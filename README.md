# ğŸš€ Custom Tool Parser for Open Source Models

Make any Openâ€‘Source LLM toolâ€‘ready in your AI SDK projectsâ€”no matter which serving framework you use.

## ğŸŒŸ Why This Exists

Many selfâ€‘hosted or thirdâ€‘party model endpoints (vLLM, MLCâ€‘LLM, Ollama, OpenRouter, etc.) donâ€™t yet expose the OpenAIâ€‘style `tools` parameter, forcing you to hack together tool parsing.  
This project provides a flexible middleware that:

- Parses tool calls from streaming or batch responses  
- Supports Hermes and Gemma formats  
- Llama, Mistral, and JSON formats are coming soon  
- Handles interleaved or â€œchattyâ€ tool calls  
- Corrects common issues in small models (extra markdown fences, semicolon separators)

## ğŸ”§ Installation

```bash
pnpm install @ai-sdk-tool/parser
```

## ğŸ¯ Quickstart

1. Wrap your model with the provided middleware  
2. Stream or generate text as usual  
3. Intercept tool calls as `tool-result` events

---

## ğŸ”Œ Example: Hermesâ€‘Style Middleware

See `examples/src/hermes-middleware-example.ts` for the full demo:

```typescript
// filepath: examples/src/hermes-middleware-example.ts
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { wrapLanguageModel, streamText } from 'ai';
import { hermesToolMiddleware } from './hermes-middleware';

const openrouter = createOpenAICompatible({ /* ... */ });

async function main() {
  const result = streamText({
    model: wrapLanguageModel({
      model: openrouter('google/gemma-3-27b-it'),
      middleware: hermesToolMiddleware,
    }),
    system: 'You are a helpful assistant.',
    prompt: 'What is the weather in my city?',
    maxSteps: 4,
    tools: {
      get_location: { /* ... */ },
      get_weather: { /* ... */ },
    },
  });

  for await (const part of result.fullStream) {
    // ...handling text-delta and tool-result...
  }
}

main().catch(console.error);
```

---

## ğŸ¤ Contributing

â€¢ Feel free to open issues or PRsâ€”especially for new model formats.  
â€¢ See `CONTRIBUTING.md` for guidelines.
