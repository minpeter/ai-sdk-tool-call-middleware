#!/usr/bin/env tsx
/**
 * Regression Benchmark Runner
 *
 * Runs specialized benchmarks for different models and protocols:
 * 1. Qwen/Qwen3-235B-A22B-Instruct-2507: Native vs Gemma vs Hermes
 * 2. zai-org/GLM-4.6: Native vs MorphXML vs YamlXML
 * 3. deepseek-ai/DeepSeek-R1-0528: MorphXML vs YamlXML vs Gemma vs Hermes (Native excluded)
 */

import { execSync } from "node:child_process";
import fs from "node:fs";
import path from "node:path";
import { createOpenAICompatible } from "@ai-sdk/openai-compatible";
import {
  bfclMultipleBenchmark,
  bfclParallelBenchmark,
  bfclParallelMultipleBenchmark,
  bfclSimpleBenchmark,
  evaluate,
} from "@ai-sdk-tool/eval";
import { createDiskCacheMiddleware } from "@ai-sdk-tool/middleware";
import {
  gemmaToolMiddleware,
  hermesToolMiddleware,
  morphXmlToolMiddleware,
  orchestratorToolMiddleware,
} from "@ai-sdk-tool/parser";
import {
  extractReasoningMiddleware,
  type LanguageModel,
  wrapLanguageModel,
} from "ai";

const QWEN_MODEL = "Qwen/Qwen3-235B-A22B-Instruct-2507";
const GLM_MODEL = "zai-org/GLM-4.6";
const DEEPSEEK_MODEL = "deepseek-ai/DeepSeek-R1-0528";

const diskCacheMiddleware = createDiskCacheMiddleware({
  cacheDir: ".benchmark-results/cache",
});

const commitHash = execSync("git rev-parse HEAD").toString().trim();
const shortHash = commitHash.slice(0, 7);
const branch =
  process.env.GITHUB_HEAD_REF ||
  process.env.GITHUB_REF_NAME ||
  execSync("git rev-parse --abbrev-ref HEAD").toString().trim();

console.log(
  `üîç Running specialized regression benchmarks for commit ${shortHash}`
);
console.log(`üì¶ Branch: ${branch}\n`);

if (!process.env.FRIENDLI_TOKEN) {
  console.error("‚ùå ERROR: FRIENDLI_TOKEN environment variable is not set");
  process.exit(1);
}

const friendli = createOpenAICompatible({
  name: "friendli.serverless",
  apiKey: process.env.FRIENDLI_TOKEN,
  baseURL: "https://api.friendli.ai/serverless/v1",
});

function createWrappedModel(
  baseModel: LanguageModel,
  middleware: any[] = []
): LanguageModel {
  return wrapLanguageModel({
    model: baseModel,
    middleware: [
      ...middleware,
      extractReasoningMiddleware({ tagName: "think" }),
      diskCacheMiddleware,
    ],
  });
}

interface BenchmarkResult {
  commit: string;
  branch: string;
  timestamp: string;
  mode: "fast" | "full";
  results: {
    qwen: Record<string, Record<string, number>>;
    glm: Record<string, Record<string, number>>;
    deepseek: Record<string, Record<string, number>>;
  };
}

const allBenchmarks = [
  bfclSimpleBenchmark,
  bfclMultipleBenchmark,
  bfclParallelBenchmark,
  bfclParallelMultipleBenchmark,
];

async function runModelBenchmark(
  modelId: string,
  configs: Record<string, LanguageModel>,
  benchmarks: any[]
): Promise<Record<string, Record<string, number>>> {
  console.log(`\nüöÄ Testing ${modelId}...`);
  const results = await evaluate({
    models: configs,
    benchmarks,
    reporter: "console.summary",
    temperature: 0.0,
    maxTokens: 1024,
  });

  const scores: Record<string, Record<string, number>> = {};
  for (const name of Object.keys(configs)) {
    scores[name] = {};
  }

  for (const result of results) {
    const modelName = result.modelKey;
    if (modelName && scores[modelName]) {
      scores[modelName][result.benchmark] = result.result.score;
    }
  }

  return scores;
}

async function main() {
  const timestamp = new Date().toISOString();
  const mode = (process.env.BENCHMARK_MODE || "fast") as "fast" | "full";
  const limit = mode === "fast" ? 5 : undefined;

  if (limit) {
    process.env.BFCL_LIMIT = limit.toString();
  } else {
    process.env.BFCL_LIMIT = undefined;
  }

  try {
    // 1. Qwen Benchmarks
    const qwenScores = await runModelBenchmark(
      QWEN_MODEL,
      {
        native: createWrappedModel(friendli(QWEN_MODEL)),
        gemma: createWrappedModel(friendli(QWEN_MODEL), [gemmaToolMiddleware]),
        hermes: createWrappedModel(friendli(QWEN_MODEL), [
          hermesToolMiddleware,
        ]),
      },
      allBenchmarks
    );

    // 2. GLM Benchmarks
    const glmScores = await runModelBenchmark(
      GLM_MODEL,
      {
        native: createWrappedModel(friendli(GLM_MODEL)),
        morphxml: createWrappedModel(friendli(GLM_MODEL), [
          morphXmlToolMiddleware,
        ]),
        yamlxml: createWrappedModel(friendli(GLM_MODEL), [
          orchestratorToolMiddleware,
        ]),
      },
      allBenchmarks
    );

    // 3. DeepSeek Benchmarks
    const deepseekScores = await runModelBenchmark(
      DEEPSEEK_MODEL,
      {
        morphxml: createWrappedModel(friendli(DEEPSEEK_MODEL), [
          morphXmlToolMiddleware,
        ]),
        yamlxml: createWrappedModel(friendli(DEEPSEEK_MODEL), [
          orchestratorToolMiddleware,
        ]),
        gemma: createWrappedModel(friendli(DEEPSEEK_MODEL), [
          gemmaToolMiddleware,
        ]),
        hermes: createWrappedModel(friendli(DEEPSEEK_MODEL), [
          hermesToolMiddleware,
        ]),
      },
      allBenchmarks
    );

    const fullResult: BenchmarkResult = {
      commit: commitHash,
      branch,
      timestamp,
      mode,
      results: {
        qwen: qwenScores,
        glm: glmScores,
        deepseek: deepseekScores,
      },
    };

    const resultsDir = path.join(process.cwd(), ".benchmark-results");
    if (!fs.existsSync(resultsDir))
      fs.mkdirSync(resultsDir, { recursive: true });

    const filename = `benchmark-${shortHash}-${Date.now()}.json`;
    fs.writeFileSync(
      path.join(resultsDir, filename),
      JSON.stringify(fullResult, null, 2)
    );

    if (process.env.CI) {
      fs.appendFileSync(
        path.join(resultsDir, "history.jsonl"),
        `${JSON.stringify(fullResult)}\n`
      );
    }

    console.log("\n" + "‚ïê".repeat(80));
    console.log("üìä SPECIALIZED REGRESSION TEST REPORT");
    console.log("‚ïê".repeat(80));

    const colors = {
      green: "\x1b[32m",
      red: "\x1b[31m",
      gray: "\x1b[90m",
      reset: "\x1b[0m",
    };

    const printTable = (
      title: string,
      scores: Record<string, Record<string, number>>
    ) => {
      console.log(`\n### ${title}`);
      const protocols = Object.keys(scores);
      const benchmarks = Object.keys(scores[protocols[0]] || {});
      if (benchmarks.length === 0) return;

      const hasNative = protocols.includes("native");
      const colWidths = {
        benchmark: 23,
        protocol: 10,
        diff: 8,
      };

      const headers = hasNative
        ? protocols.flatMap((p) => (p === "native" ? [p] : [p, "Œî"]))
        : protocols;

      const headerWidths = hasNative
        ? protocols.flatMap((p) =>
            p === "native"
              ? [colWidths.protocol]
              : [colWidths.protocol, colWidths.diff]
          )
        : protocols.map(() => colWidths.protocol);

      console.log(
        `‚îå${"‚îÄ".repeat(colWidths.benchmark + 2)}‚î¨${headerWidths.map((w) => "‚îÄ".repeat(w + 2)).join("‚î¨")}‚îê`
      );
      console.log(
        `‚îÇ ${"Benchmark".padEnd(colWidths.benchmark)} ‚îÇ ${headers.map((h, i) => h.padEnd(headerWidths[i])).join(" ‚îÇ ")} ‚îÇ`
      );
      console.log(
        `‚îú${"‚îÄ".repeat(colWidths.benchmark + 2)}‚îº${headerWidths.map((w) => "‚îÄ".repeat(w + 2)).join("‚îº")}‚î§`
      );

      for (const b of benchmarks) {
        const nativeScore = hasNative ? scores.native[b] : 0;
        const cells = hasNative
          ? protocols.flatMap((p) => {
              const score = `${(scores[p][b] * 100).toFixed(1)}%`.padStart(
                colWidths.protocol
              );
              if (p === "native") return [score];
              const diff = scores[p][b] - nativeScore;
              const diffNum = (diff * 100).toFixed(0);
              let diffStr: string;
              if (diff > 0) {
                diffStr = `${colors.green}+${diffNum}%${colors.reset}`.padStart(
                  colWidths.diff + colors.green.length + colors.reset.length
                );
              } else if (diff < 0) {
                diffStr = `${colors.red}${diffNum}%${colors.reset}`.padStart(
                  colWidths.diff + colors.red.length + colors.reset.length
                );
              } else {
                diffStr = `${colors.gray}0%${colors.reset}`.padStart(
                  colWidths.diff + colors.gray.length + colors.reset.length
                );
              }
              return [score, diffStr];
            })
          : protocols.map((p) =>
              `${(scores[p][b] * 100).toFixed(1)}%`.padStart(colWidths.protocol)
            );
        console.log(
          `‚îÇ ${b.padEnd(colWidths.benchmark)} ‚îÇ ${cells.join(" ‚îÇ ")} ‚îÇ`
        );
      }

      console.log(
        `‚îî${"‚îÄ".repeat(colWidths.benchmark + 2)}‚î¥${headerWidths.map((w) => "‚îÄ".repeat(w + 2)).join("‚î¥")}‚îò`
      );
    };

    printTable(`QWEN (${QWEN_MODEL})`, qwenScores);
    printTable(`GLM (${GLM_MODEL})`, glmScores);
    printTable(`DEEPSEEK (${DEEPSEEK_MODEL})`, deepseekScores);

    process.exit(0);
  } catch (error) {
    console.error("‚ùå Benchmark failed:", error);
    process.exit(1);
  }
}

main();
